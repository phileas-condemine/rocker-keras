Keras_demo
========================================================

author: AGPC
date: `r as.Date(Sys.time())`
autosize: true

About this presentation
========================================================

This presentation should be read after the ppt slides

You will see that Keras is like a toolbox.

Most tools were described in research papers and implemented in specific situation.

But given high resilience of the API, you can mix all types or layers and find new NN recipes.

Serendipity is indeed a frequent word in deep learning.


Install Keras for RStudio
========================================================

```{r,eval=F}
install.packages("keras")
keras::install_keras()
```

Error: Prerequisites for installing TensorFlow not available.

Please install the following Python packages before proceeding: pip, virtualenv

=> https://www.saltycrane.com/blog/2010/02/how-install-pip-ubuntu/



Packages
========================================================
```{r echo=F , warnings=F}
if(!require("devtools")){
install.packages("devtools")
}
devtools::install_github('hadley/ggplot2')
library(ggplot2)
packages_list=c("codetools","knitr","tidyverse","keras","plotly","webshot")
for (pkg in packages_list){
  print(paste0("check: ",pkg))
if(!require(pkg,character.only = T)){
  print(paste0("need to install: ",pkg))
  install.packages(pkg)
}
library(pkg,character.only = T)
}

```


Load data
========================================================
```{r}
path_house_price_data <- system("find ./ -name house_price_data_for_keras.RData",intern = T)
load(path_house_price_data)
head(x_train)
head(y_train)
head(x_test)
head(y_test)
```

Define params
========================================================
There is a slide dedicated to batch size selection
```{r}
batch_size <- 4 #if batch_size is too low, it's like SGD, loss function is very unstable. if batch_size is too high, the gradient will be almost null after a very few iterations.
epochs <- 10
```

Design a neural network
========================================================

```{r}
model <- keras_model_sequential()

model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')
```
<p color="red">
Using TensorFlow backend.

2017-11-02 12:32:56.960563: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
</p>
https://github.com/rstudio/keras/issues?q=is%3Aissue+is%3Aclosed

Choose loss function and optimizer
========================================================
```{r}
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam',#try also sgd
  metrics = c('mean_squared_error')
)
```

Fit the model and keep track of the optimization process
========================================================
```{r}
history <- model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 2,
  validation_split = 0.3,callbacks=list(callback_tensorboard())
)
```

Evaluate the model
========================================================
```{r}
score <- model %>% evaluate(
  x_test, y_test,
  batch_size = batch_size,
  verbose = 1
)

print(paste0("RMSE on test set ",sqrt(score$mean_squared_error)))
```

Encapsulate everything in a function
========================================================
the function returns Model History Score (a list of 3 objects) MHS
```{r}
path_run_model <- system("find /home/rstudio -name run_model.R",intern=T)
source(path_run_model)
MHS <- build_model(x_train,y_train,x_test,y_test)
```

Add layers
========================================================
```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

path_compile_evaluate_model <- system("find /home/rstudio -name compile_evaluate_model.R",intern=T)
source(path_compile_evaluate_model)
MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN3Layer100relu80relu60relu_batch1.RData")
```
```{r}
load("NN3Layer100relu80relu60relu_batch1.RData")
plot(MHS[[2]])
```

Add Dropout
========================================================
```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_activation(activation = 'relu') %>%
  layer_dropout(.7)%>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN3Layer100relu80relu60relu_batch1_dropout70.RData")
```
```{r}
load("NN3Layer100relu80relu60relu_batch1_dropout70.RData")
plot(MHS[[2]])
```

Use alpha Dropout instead
========================================================

```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_activation(activation = 'relu') %>%
  layer_alpha_dropout(.7)%>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN3Layer100relu80relu60relu_batch1_alphadropout70.RData")
```
```{r}
load("NN3Layer100relu80relu60relu_batch1_alphadropout70.RData")
plot(MHS[[2]])
```

Use gaussian Dropout instead
========================================================

```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_gaussian_dropout(.7)%>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN3Layer100relu80relu60relu_batch4_Den1_gaussiandropout70_Act1.RData")
```
Den1 Act1 GDO "RMSE on test set 0.187559564216181"

Den1 GDO Act1 "RMSE on test set 0.158125156800799-0.166876005632993"

```{r}
load("NN3Layer100relu80relu60relu_batch4_Den1_gaussiandropout70_Act1.RData")
plot(MHS[[2]])
```

Reduce overfitting - Gaussian Noise
========================================================

```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_activation(activation = 'relu') %>%
  layer_gaussian_noise(stddev = 1) %>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN3Layer100relu80relu60relu_batch4_GN1_betweenDen1Act1.RData")
```
[1] "RMSE on test set 0.188398698260064" better !

[1] "RMSE on test set 0.250187371099559" if you put it between Dense 1 and Activation 1.
```{r}
load("NN3Layer100relu80relu60relu_batch4_GN1.RData")
plot(MHS[[2]])
```

Activity Regularizer
========================================================

Have a look at : https://keras.io/regularizers/
```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_activity_regularization(l1=0,l2=1)%>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN3Layer100relu80relu60relu_batch4_Den1_AR0100_Act1.RData")
```
Den1 AR7030 Act1 200 iter "RMSE on test set 0.269866256451131"

Den1 AR1000 Act1 500 iter "RMSE on test set 0.373504576873097 - 0.559613751871897"

Den1 AR0100 Act1 500 iter "RMSE on test set 0.234313469190436 - 0.264217724830065"

Act1 AR7030 Den2 "RMSE on test set 0.54332276985586"
```{r}
load("NN3Layer100relu80relu60relu_batch4_Den1_AR0100_Act1.RData")
plot(MHS[[2]])
```

Batch Normalization
========================================================
Have a look at : https://keras.io/regularizers/

There is a ton of paramaters : `help(layer_batch_normalization)`

```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100,input_shape = ncol(x_train)) %>%
  layer_batch_normalization()%>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN3Layer100relu80relu60relu_batch4_BN_betweenDen1Act1.RData")

```

BN_betweenDen1Act1 "RMSE on test set 0.256533278504706"

BN_betweenAct1Den2 "RMSE on test set 1.12236581801725"

Care with the interpretation of the documentation "Normalize the activations of the previous layer at each batch"


```{r}
load("NN3Layer100relu80relu60relu_batch4_BN_betweenDen1Act1.RData")
plot(MHS[[2]])

```

Mix GDO & GN & BN
========================================================
```{r}
batch_size <- 24 #if batch_size is too low, it's like SGD, loss function is very unstable. if batch_size is too high, the gradient will be almost null after a very few iterations.
epochs <- 1000
```
```{r,eval=F}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 100, input_shape = ncol(x_train)) %>%
  layer_gaussian_dropout(.7)%>%
  layer_activation(activation = 'relu') %>%
  layer_gaussian_noise(stddev = 1) %>%
  layer_batch_normalization()%>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 80) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 100) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units=1)%>%
  layer_activation(activation = 'linear')

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)
save(list="MHS",file="NN5Layer100relu80relu60relu80relu100relu_batch24_GDO70_Act1_GN1_BN.RData")
```
Den1 BN Act1 GN1 : "RMSE on test set 0.182853024983419"

Act1 GN1 BN : "RMSE on test set 0.21932565772734"

Den1 GDO70 Act 1 GN1 BN "RMSE on test set 0.184909205825903 - 0.14411899553965"


Impact of batch size
========================================================
```{r eval=FALSE}
NN_list_batch_size <- c("NN5Layer100relu80relu60relu80relu100relu_batch2_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch4_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch6_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch8_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch10_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch12_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch16_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch20_GDO70_Act1_GN1_BN.RData"
                        ,"NN5Layer100relu80relu60relu80relu100relu_batch24_GDO70_Act1_GN1_BN.RData")
perf <- data.frame()

for (nm in NN_list_batch_size){
  print(nm)
  load(nm)
  batch_size <- regexpr(pattern = "batch[[:digit:]]*",nm)
  batch_size <- substr(nm,start = batch_size[1],stop=batch_size[1]+attr(batch_size,"match.length")-1)
  metrics <- MHS[[2]][2]$metrics
  sub_perf <- data.frame(iter=1:length(metrics$val_mean_squared_error),perf_train=metrics$mean_squared_error,perf_val=metrics$val_mean_squared_error,lr=metrics$lr,batch=batch_size)
  perf <- rbind(perf,sub_perf)
}

g <- ggplot(perf[perf$iter>10,])+geom_line(aes(x=iter,y=perf_train,color=batch))
g <- ggplotly(g)
htmlwidgets::saveWidget(as_widget(g), file = "gg_perf_train.html")

g <- ggplot(perf[perf$iter>10,])+geom_line(aes(x=iter,y=perf_val,color=batch))
g <- ggplotly(g)
htmlwidgets::saveWidget(as_widget(g), file = "gg_perf_val.html")

g <- ggplot(perf[perf$iter>10,])+geom_line(aes(x=iter,y=lr,color=batch))
g <- ggplotly(g)
htmlwidgets::saveWidget(as_widget(g), file = "gg_lr.html")
```


Callbacks
========================================================
Early stop

Reduce learning rate

Tensorboard

```{r,eval=F}
history <- model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 2,
  validation_split = 0.3,
  callbacks=list(callback_tensorboard(log_dir = "logs/",write_graph = TRUE,write_images = TRUE,write_grads = TRUE,),
                 callback_early_stopping(monitor = "val_loss",patience=100,mode = "min"),
                 callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.8,patience=10,verbose=1)))
```


Tensorboard
========================================================
docker exec -it keras_demo pip install tensorboard

Run it inside the docker, then need to forward it to your browser...
docker exec -it keras_demo tensorboard --logdir=logs

then http://172.17.0.2:6006 this IP is local (inside the container)

Run it locally from the saved logs
tensorboard --logdir=~/Documents/training\ hackathon\ 2017/logs

then http://localhost:6006

Word embedding - data selection
========================================================
```{r data selection, eval=F}

prune_freq_words=1
max_features=800
maxlen = 50


path_data <- system(sprintf("find /home/rstudio -name %s","data_prepared_for_keras_lstm.RData"),intern = T)
load(path_data)#this dataset was created in tm section after text cleaning

data_prepared$Value <- as.numeric(gsub("\\$","",data_prepared$Value))
data_prepared$Description=as.character(data_prepared$Description)
data_prepared=data_prepared[!data_prepared$Value==0,]

sentence_split <- strsplit(data_prepared$Description,split = " ")
nb_words <- max(unlist(lapply(sentence_split,function(x)max(as.numeric(x),na.rm = T))),na.rm=T)
nb_words
list_to_remove=setdiff(1:nb_words,seq(prune_freq_words,max_features+prune_freq_words))
list_to_remove=paste0(" ",list_to_remove," ")

a <- list_to_remove
b <- rep(" ",length(a))
names(b) <- a

data_prepared$Description <- stringr::str_replace_all(data_prepared$Description,b)
head(data_prepared$Description)
str_len <- str_length(data_prepared$Description)
data_prepared$Description <- apply(data.frame(desc=data_prepared$Description,
                                              str_len=str_len),
                                   1,
                                   function(x){
                                     substr(x[1],start = 2,stop=as.numeric(x[2])-1)
                                   }
                                   )
head(data_prepared$Description)

data_prepared=data_prepared[str_length(data_prepared$Description)>0,]

```

Word embedding - data formatting
========================================================

Running the LSTM is not difficult, but getting the data in the right format can be painful.<br>
Also this package is recent and not very used yet so it's difficult to find help on stackoverflow BUT many issues were already opened and fixed on github.<br>
```{r data selection for word embedding}


set.seed(1337)
sample_train=sample(1:nrow(data_prepared),round(0.7*nrow(data_prepared)))
y_train <- log(data_prepared[sample_train,]$Value)
y_test <- log(data_prepared[-sample_train,]$Value)

# https://github.com/rstudio/keras/issues/77
tokenizer <- text_tokenizer(num_words = max_features)
x_train <- data_prepared[sample_train,]$Description #%>%strsplit(" ")%>%lapply(as.integer)
fit_text_tokenizer(tokenizer, x_train)
x_train <- texts_to_sequences(tokenizer, x_train)

x_test <- data_prepared[-sample_train,]$Description #%>%strsplit(" ")%>%lapply(as.integer)
fit_text_tokenizer(tokenizer, x_test)
x_test <- texts_to_sequences(tokenizer, x_test)




######## PRUNE WORDS LESS FREQUENT THAT MAX_FEATURES AND MORE FREQUENT THAN PRUNE_FREQ_WORDS
# imdb <- dataset_imdb(num_words = max_features)
# head(imdb$train$x)
# 
# 
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
```

Word embedding - running the model
========================================================
Example here : https://keras.rstudio.com/articles/examples/imdb_lstm.html<br>
data prep : https://github.com/wush978/FeatureHashing/blob/master/vignettes/SentimentAnalysis.Rmd<br>

```{r run model on NN,eval=F}

batch_size = 40

model <- keras_model_sequential()
model %>%
  layer_embedding(input_dim = max_features, output_dim = 128) %>%
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
  layer_dense(units = 1, activation = 'linear')

path_compile_evaluate_model <- system("find /home/rstudio -name compile_evaluate_model.R",intern=T)
source(path_compile_evaluate_model)

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)

save(list=c("MHS"),file = "HACKATHON_DATA_NN_LSTM_DO20_RDO20.RData")




```

Word embedding - Add Convolution 1D
========================================================
Example here : https://keras.rstudio.com/articles/examples/imdb_lstm.html<br>
data prep : https://github.com/wush978/FeatureHashing/blob/master/vignettes/SentimentAnalysis.Rmd<br>


```{r adding convolution in 1 dimension}
embedding_size = 128
# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_output_size = 70

# Training
batch_size = 40
epochs = 20
model <- keras_model_sequential()
model %>%
  layer_embedding(input_dim =  max_features,output_dim = embedding_size, input_length = maxlen) %>%
  layer_dropout(0.25) %>%
  layer_conv_1d(
    filters, 
    kernel_size, 
    padding = "valid",
    activation = "relu",
    strides = 1
  ) %>%
  layer_max_pooling_1d(pool_size) %>%
  layer_lstm(units = lstm_output_size, dropout = 0.2, recurrent_dropout = 0.2) %>%
  layer_dense(1,activation = "linear") 

MHS <- compile_evaluate_model(model,x_train,y_train,x_test,y_test)

save(list=c("MHS"),file = "HACKATHON_DATA_NN_LSTM_DO25_Conv1D.RData")



model %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = "mean_squared_error"
)

# Training ----------------------------------------------------------------

model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_data = list(x_test, y_test)
)

```



Practice
========================================================
Fine tune Keras NN layers & parameters

Check the documentation and try some other layers

Try a simple NN on Seattle data

Try word embedding techniques
